{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cbook, cm\n",
    "from matplotlib.colors import LightSource\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Steepest Descent\n",
    "As a test function, we use\n",
    "\n",
    "$$\n",
    "f({\\bf x}) = 2e^{x_{1}-.1}\\cosh(3x_{2}) + e^{-x_{1}-.1}, ~ {\\bf x} = (x_{1}, x_{2})\n",
    "$$\n",
    "\n",
    "One can quickly show that this has a global minimum at ${\\bf x}_{\\ast} = (-.5\\ln(2), 0)$, and $f({\\bf x}_{\\ast})=2.5592666966582156$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_fun_plot(x1vals, x2vals):\n",
    "    ex = np.exp(x1vals - .1)\n",
    "    enx = np.exp(-x1vals - .1)\n",
    "    ey = 2.*np.cosh(3.*x2vals)\n",
    "    fun = ey[:, np.newaxis] @ ex[np.newaxis, :] + np.ones((x2vals.size,1)) @ enx[np.newaxis, :]\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_fun(xval):\n",
    "    x1, x2 = xval[0], xval[1]\n",
    "    return 2.*np.exp(x1-.1)*np.cosh(3.*x2) + np.exp(-x1-.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cvx_fun(xval):\n",
    "    x1, x2 = xval[0], xval[1]\n",
    "    gvec = np.zeros(2, dtype=np.float64)\n",
    "    gvec[0] = 2. * np.exp(x1-.1) * np.cosh(3.*x2) - np.exp(-x1-.1)\n",
    "    gvec[1] = 6. * np.exp(x1-.1) * np.sinh(3.*x2)\n",
    "    return gvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstr = .5 * np.log(.5)\n",
    "ystr = 0.\n",
    "xvals = np.linspace(xstr - 1., xstr + 1., int(1e2)+1)\n",
    "yvals = np.linspace(ystr - 1., ystr + 1., int(1e2)+1)\n",
    "test_fun = cvx_fun_plot(xvals, yvals)\n",
    "plt.contourf(xvals, yvals, test_fun, levels=40)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the steepest-descent update formula \n",
    "\n",
    "$$\n",
    "{\\bf x}_{k+1} = {\\bf x}_{k} - \\mu_{k}\\nabla f({\\bf x}_{k}), ~ \\mu_{k} > 0\n",
    "$$\n",
    "\n",
    "with the convergence constraints\n",
    "\n",
    "$$\n",
    "f({\\bf x}_{k+1}) \\lt f({\\bf x}_{k}), ~ \\sum_{k=0}^{\\infty}\\mu_{k}=\\infty, ~ \\sum_{k=0}^{\\infty}\\mu^{2}_{k}\\lt \\infty.\n",
    "$$\n",
    "\n",
    "Given some user defined tolerance, say `tol`, we iterate until \n",
    "\n",
    "$$\n",
    "\\left|\\left|\\nabla f({\\bf x}_{k})\\right|\\right| \\lt \\text{tol}.\n",
    "$$\n",
    "\n",
    "Choosing 100 initial conditions from the circle\n",
    "\n",
    "$$\n",
    "(x_{1} + .5\\ln(2))^{2} + x_{2}^{2} = 1\n",
    "$$\n",
    "\n",
    "plot the iteration count as a function of the angle along your circle of initial conditions.  Commment on the performance of your implementation in particular with regards to how you choose your update scheme for $\\mu_k$ and also how you choose your tolerance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_step(xval, gvec, muval):\n",
    "    return xval - muval * gvec\n",
    "\n",
    "def steepest_descent(x0, tol):\n",
    "    mu0 = 1.\n",
    "    cnt = 0.\n",
    "    x1tst = grad_step(x0, grad_cvx_fun(x0), mu0)\n",
    "\n",
    "    while np.linalg.norm(grad_cvx_fun(x0)) > tol: \n",
    "        # search along gradient to find a proper descent direction\n",
    "        while cvx_fun(x1tst) >= cvx_fun(x0):\n",
    "            # what are you gonna do?  \n",
    "        x0 = x1tst\n",
    "        cnt += 1\n",
    "        \n",
    "        # How are we going to update our steps to fit with our convergence requirements?  \n",
    "\n",
    "        mu0 = # what are you gonna do?  \n",
    "        \n",
    "    return x0, cnt\n",
    "\n",
    "def steep_descent_tester(num_angles, tol):\n",
    "    x1str = .5 * np.log(.5)\n",
    "    x2str = 0.\n",
    "    \n",
    "    angles = np.linspace(0, 2.*np.pi, num_angles)\n",
    "    counts = np.zeros(num_angles, dtype=int)\n",
    "    for jj in range(num_angles):\n",
    "        x0 = np.array([ x1str + np.cos(angles[jj]), x2str + np.sin(angles[jj]) ]) \n",
    "        xstr, cnt = steepest_descent(x0, tol)\n",
    "        counts[jj] = cnt\n",
    "\n",
    "    plt.plot(angles, counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steep_descent_tester(100, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell me a story :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Momentum to Accelerate Convergence\n",
    "\n",
    "So we can imagine that as the steepest descent method starts to converge onto a minimum of $f({\\bf x})$, it faces a landscape in which $\\nabla f({\\bf x})$ doesn't change much from iteration to iteration.  To take advantage of this, we add what is called *momentum* to our steepest descent update formula.  This looks like the following algorithm.\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta {\\bf x}_{k+1} =  & a\\Delta {\\bf x}_{k} - \\mu \\nabla f({\\bf x}_{k})\\\\\n",
    "{\\bf x}_{k+1} = & {\\bf x}_{k} + \\Delta {\\bf x}_{k+1}.\n",
    "\\end{align*}\n",
    "\n",
    "Here, $0<a<1$ is the *momentum term* and we now fix the step $\\mu$ to be a constant (though we still need to make sure we're taking descent directions so we need to be careful about choices for this parameter).  Since $\\mu$ is now a constant, we call it the *learning rate* of our descent method.    \n",
    "\n",
    "* Show that over some **very large** number of iterations such that $\\nabla f({\\bf x}_{k})\\approx {\\bf c}$, we can solve for $\\Delta {\\bf x}$ so that\n",
    "$$\n",
    "\\Delta {\\bf x} \\approx -\\frac{\\mu}{1-a}{\\bf c}\n",
    "$$\n",
    "* What effect does the momentum then have on your method?  Why does adding momentum in effect automatically make your method have an adaptive step-size?  \n",
    "* To implement this method, we suppose that $\\Delta {\\bf x}_{0}=0$ and we then make a choice of $a$ and $\\mu$.  Code this up and explore how the iteration counts can be improved by using different choices of $\\mu$ and $a$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_solver(a, mu, xj, dxj):\n",
    "    dxjp1 = # what are you gonna do? \n",
    "    xjp1 = # what are you gonna do?  \n",
    "    return xjp1, dxjp1\n",
    "    \n",
    "def momentum_tester(num_angles, a, mu, tol):\n",
    "    x1str = .5 * np.log(.5)\n",
    "    x2str = 0.\n",
    "    \n",
    "    angles = np.linspace(0, 2.*np.pi, num_angles)\n",
    "    counts = np.zeros(num_angles, dtype=int)\n",
    "    \n",
    "    for jj in range(num_angles):\n",
    "        x0 = np.array([ x1str + np.cos(angles[jj]), x2str + np.sin(angles[jj]) ]) \n",
    "        dx0 = np.zeros(2, dtype=np.float64)    \n",
    "        x1, dx1 = momentum_solver(a, mu, x0, dx0)\n",
    "        cnt = 0\n",
    "        while np.linalg.norm(x1-x0) >= tol * np.linalg.norm(x0):\n",
    "            x0, dx0 = x1, dx1\n",
    "            x1, dx1 = momentum_solver(a, mu, x0, dx0)\n",
    "            cnt += 1\n",
    "        \n",
    "        counts[jj] = cnt\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell me a story :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (Graduate): Epigraphs\n",
    "\n",
    "For a function $f:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}$, we define its *epigraph* $\\text{epi}(f;C)$ to be \n",
    "\n",
    "$$\n",
    "\\text{epi}(f;C) = \\left\\{ ({\\bf x}, r)\\in C \\times \\mathbb{R}: ~ f({\\bf x}) \\leq r \\right\\},\n",
    "$$\n",
    "\n",
    "where $C\\subset \\mathbb{R}^{n}$ is convex.  Show that $f$ is a convex function if and only if $\\text{epi}(f)$ is a convex subset of $\\mathbb{R}^{n}\\times \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (Graduate): $l_{1}$-norms and Sparsity\n",
    "\n",
    "Suppose that, for ${\\bf y}, {\\bf z} \\in \\mathbb{R}^{n}$ we want to solve \n",
    "\n",
    "$$\n",
    "{\\bf z}_{\\ast} = \\text{arg min}_{{\\bf z}} ~ \\left|\\left|{\\bf y} - A{\\bf z}\\right|\\right|_{2}^{2} + \\lambda \\left|\\left|{\\bf z}\\right|\\right|_{1} + \\gamma \\left|\\left|{\\bf z}\\right|\\right|_{2}^{2}\n",
    "$$\n",
    "\n",
    "where $\\lambda, ~ \\gamma > 0$ and for real-valued, square $A$ such that $A^{T}A = AA^{T} = I$.  Letting ${\\bf u} = A^{T}{\\bf y}$, show that \n",
    "\n",
    "$$\n",
    "z_{\\ast,l} = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "\\frac{1}{1+\\gamma}\\left(u_{l} - \\frac{\\lambda}{2}\\right) & u_{l} > \\frac{\\lambda}{2} \\\\\n",
    "0 & |u_{l}| \\leq \\frac{\\lambda}{2} \\\\ \n",
    "\\frac{1}{1+\\gamma}\\left(u_{l} + \\frac{\\lambda}{2}\\right) & u_{l} < -\\frac{\\lambda}{2} \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Show that this multi-part formula can be written in the freakishly compact form\n",
    "\n",
    "$$\n",
    "z_{\\ast,l} = \\frac{u_{l}}{1+\\gamma}\\text{max}\\left\\{0, 1 - \\frac{\\lambda}{2|u_{l}|}\\right\\}\n",
    "$$\n",
    "\n",
    "Note, to do all of this, you'll need the sub-differential of $|x|$, $\\partial |x|$, where \n",
    "\n",
    "$$\n",
    "\\partial |x| = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "1 & x > 0\\\\\n",
    "r\\in[-1,1]& x=0, ~ \\\\\n",
    "-1 & x < 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "What role then does the $l_{1}$-norm play in this problem and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some data in R^3 for the Perceptron to classify.  \n",
    "def hyperplane(x):\n",
    "    return 1. + np.sum(np.ones(3) * x) # let's keep this simple\n",
    "    \n",
    "# Generate labeled data on either side of a 3D plane.\n",
    "def label_maker(Npts, margin):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    points_above = []\n",
    "    labels_plus = []    \n",
    "    points_below = []\n",
    "    labels_minus = []\n",
    "\n",
    "    for jj in range(Npts):\n",
    "        tst_point = 10.* np.random.randn(3)\n",
    "        find = True\n",
    "        while find:\n",
    "            if hyperplane(tst_point) >= margin:\n",
    "                points_above.append(tst_point)\n",
    "                labels_plus.append(1)\n",
    "                find = False\n",
    "                ax.scatter(tst_point[0], tst_point[1], tst_point[2], c='k')\n",
    "            elif hyperplane(tst_point) <= -margin:\n",
    "                points_below.append(tst_point)\n",
    "                labels_minus.append(-1)\n",
    "                find = False\n",
    "                ax.scatter(tst_point[0], tst_point[1], tst_point[2], c='r')\n",
    "            else:\n",
    "                tst_point = 10.* np.random.randn(3)\n",
    "\n",
    "    xvals = np.linspace(-50., 50., int(1e1)+1)\n",
    "    yvals = np.linspace(-50., 50., int(1e1)+1)\n",
    "    xx, yy = np.meshgrid(xvals, yvals)\n",
    "    zvals = -1 - xx - yy\n",
    "    ax.plot_surface(xx, yy, zvals, linewidth=0, alpha=.25)\n",
    "    return points_above, labels_plus, points_below, labels_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_above, labels_plus, points_below, labels_minus = label_maker(1000, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the Perceptron is the simplest neural network, which for labeled data $\\left\\{({\\bf x}_{m},l_{m})\\right\\}_{m=1}^{N_{D}}$ is defined by the loss function:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}(\\theta, b) = \\frac{1}{N_{D}}\\sum_{m=1}^{N_{D}}\\text{max}\\left(0, -l_{m}\\left(b + \\left<\\theta,{\\bf x}_{m}\\right>\\right)\\right)\n",
    "$$\n",
    "\n",
    "To make the next bit of analysis more straightforward, define $\\tilde{{\\bf x}}_{m} = ({\\bf x}_{m}, 1)$ and $\\tilde{\\theta} = (\\theta, b)$ so that \n",
    "$$\n",
    "\\tilde{\\mathcal{L}}(\\theta, b) = \\frac{1}{N_{D}}\\sum_{m=1}^{N_{D}}\\text{max}\\left(0, -\\left<\\tilde{\\theta},l_{m}\\tilde{{\\bf x}}_{m}\\right> \\right)\n",
    "$$\n",
    "\n",
    "Likewise, define $\\mathcal{L}(\\tilde{\\theta}; (\\tilde{{\\bf x}}_{m}, l_{m}))$ as \n",
    "$$\n",
    "\\mathcal{L}(\\tilde{\\theta}; (\\tilde{{\\bf x}}_{m}, l_{m})) = \\text{max}\\left(0, -\\left<\\tilde{\\theta},l_{m}\\tilde{{\\bf x}}_{m}\\right> \\right)\n",
    "$$\n",
    "\n",
    "**Part 1**: Show that $\\partial_{\\tilde{\\theta}} \\mathcal{L}(\\tilde{\\theta}; (\\tilde{{\\bf x}}_{m}, l_{m}))$ is given by\n",
    "$$\n",
    "\\partial_{\\tilde{\\theta}} \\mathcal{L}(\\tilde{\\theta}; (\\tilde{{\\bf x}}_{m}, l_{m})) = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "    0 & \\left<\\tilde{\\theta},l_{m}\\tilde{{\\bf x}}_{m}\\right>   \\gt 0 \\\\\n",
    "    & \\\\\n",
    "    -l_{m}\\tilde{{\\bf x}}_{m} & \\left<\\tilde{\\theta},l_{m} \\tilde{{\\bf x}}_{m}\\right> \\lt 0\\\\\n",
    "    & \\\\\n",
    "    \\frac{1}{2}(r-1)l_{m}\\tilde{{\\bf x}}_{m} & \\left<\\tilde{\\theta},l_{m} \\tilde{{\\bf x}}_{m}\\right> = 0, ~ -1\\leq r \\leq 1\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "To do this, remember that\n",
    "$$\n",
    "\\mathcal{L}(\\tilde{\\theta}; (\\tilde{{\\bf x}}_{m}, l_{m})) = -\\frac{1}{2}\\left<\\tilde{\\theta},l_{m}\\tilde{{\\bf x}}_{m}\\right> + \\frac{1}{2}\\left|\\left<\\tilde{\\theta},l_{m}\\tilde{{\\bf x}}_{m}\\right> \\right|\n",
    "$$\n",
    "and then remember back to how we differentiated $|x|$ and then use the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**: Choosing $r=-1$, show that the steepest-descent step becomes \n",
    "\n",
    "$$\n",
    "\\tilde{\\theta}_{k+1} = \\tilde{\\theta}_{k} + \\frac{\\mu_{k}}{N_{D}}\\sum_{m=1}^{N_{D}}l_{m}\\tilde{{\\bf x}}_{m} I_{(-\\infty, 0]}\\left(\\left<\\tilde{\\theta}_{k},l_{m}\\tilde{{\\bf x}}_{m}\\right>\\right), ~ \\mu_{k} \\gt 0,\n",
    "$$\n",
    "\n",
    "where $I_{(-\\infty, 0]}(t)$ is the indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3**: Using the data generation method above, implement the steepest-descent method for the Perceptron.  Does the number of iterations the method takes to converge change based on your choice of $\\mu_{k}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd = len(points_above) + len(points_below)\n",
    "dim = points_above[0].size\n",
    "data_mat = np.ones((dim+1, Nd))\n",
    "data_mat[:dim, :len(points_above)] = # what are you gonna do?  \n",
    "data_mat[dim, :len(points_above)] = # what are you gonna do?  \n",
    "data_mat[:dim, len(points_above):] = # what are you gonna do?  \n",
    "data_mat[dim, len(points_above):] = # what are you gonna do?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_step(theta0, data_mat, mu):\n",
    "    innerp = # what are you gonna do?  \n",
    "    indskp = innerp <= 0.\n",
    "    grad = # what are you gonna do?  \n",
    "    theta1 = theta0 + mu*grad[:,np.newaxis]\n",
    "    return theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1.\n",
    "tol = 1e-3\n",
    "\n",
    "theta0 = np.zeros((dim+1, 1), dtype=np.float64)\n",
    "theta1 = perceptron_step(theta0, data_mat, mu)\n",
    "cnt = 0\n",
    "\n",
    "while np.linalg.norm(theta1-theta0) >= tol * np.linalg.norm(theta0):\n",
    "    theta0 = theta1\n",
    "    theta1 = perceptron_step(theta0, data_mat, mu)\n",
    "    cnt += 1\n",
    "\n",
    "print(theta1)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell me a story here :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_596_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
