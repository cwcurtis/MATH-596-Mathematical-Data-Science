{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6111e336",
   "metadata": {},
   "source": [
    "## Homework Six, Due 10/17/25, by 11:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bb4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94dbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1, vals2, vals3 = [], [], []\n",
    "with open(\"redshift.dat\", 'r') as f:\n",
    "    for line in f:\n",
    "        vals = line.split()\n",
    "        vals1.append(float(vals[0]))\n",
    "        vals2.append(float(vals[1]))\n",
    "        vals3.append(float(vals[2]))\n",
    "vals1 = np.array(vals1)\n",
    "vals2 = np.array(vals2)\n",
    "vals3 = np.array(vals3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fa819",
   "metadata": {},
   "source": [
    "According to Larry Wasserman, the author of the very famous \"All of Statistics\", the following is redshift data that one gets from measuring something or other about galaxies (I don't know; he doesn't give much detail aside from just saying this is about space; see https://www.stat.cmu.edu/~larry/all-of-statistics/index.html to see what I mean.)  Anyway, we can at least visualize the data in a straightforward fashion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(vals1, vals2, s=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(vals1, vals3, s=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cf5a5",
   "metadata": {},
   "source": [
    "A natural question that emerges then is how to characterize the data set `vals2` (which again has something to do with space).  To do that, and thus explore the bias/variance tradeoff we've beeen discussing in class, we generate a histogram of `vals2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b09c31",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "To define the problem, we suppose that we have data $D = \\left\\{X_{j}\\right\\}_{j=1}^{N_{D}}$ where are i.i.d. and $X_{j}\\sim p(x)$, where $p(x)$ is some unknown distribution.  A _histogram_, say $\\hat{p}_{H}(x)$ is a model/estimator of $p(x)$.  \n",
    "\n",
    "To construct it, if we know $x\\in [a,b]$, we divide $[a,b]$ into $K$ bins of width $\\delta x  = (b-a)/K$ so that \n",
    "\n",
    "$$\n",
    "[a,b]=\\cup_{l=1}^{K}[a_{l},b_{l}), ~ a_{1}=a, ~ b_{K} = b, ~b_{l}=a_{l+1}, ~ b_{l}-a_{l} = \\delta x\n",
    "$$ \n",
    "\n",
    "Define an _indicator_ function for a set $A$ as $I_{A}(x)$ so that \n",
    "\n",
    "$$\n",
    "I_{A}(x) = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "1 & x \\in A \\\\ \n",
    "0 & x \\notin A\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Then $\\hat{p}_{H}(x)$ is defined to be\n",
    "\n",
    "$$\n",
    "\\hat{p}_{H}(x) = \\frac{1}{\\delta x} \\sum_{l=1}^{K} \\hat{p}_{l} I_{[a_{l},b_{l})}(x), ~ \\hat{p}_{l} = \\frac{N_{l}}{N_{D}}\n",
    "$$\n",
    "\n",
    "where $N_{l}$ is the number of data points in the $l^{th}$-bin, i.e.\n",
    "\n",
    "$$\n",
    "N_{l} = \\sum_{j=1}^{N_{D}}I_{[a_{l},b_{l})}(X_{j}),\n",
    "$$\n",
    "\n",
    "so that $\\sum_{l=1}^{K}N_{l} = N_{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31646286",
   "metadata": {},
   "source": [
    "**Problem 1**: Using the `vals2` data set:\n",
    "\n",
    "* Plot the histogram (use `plt.hist`) using $K=2$.\n",
    "* Plot the histogram (use `plt.hist`) using $K=20$.\n",
    "* Plot the histogram (use `plt.hist`) using $K=200$.\n",
    "\n",
    "How would you describe the differences in these plots in terms of bias and variance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40cf0d-249e-4d5b-82ed-dbb3d7e908b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vals2, bins=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82582e",
   "metadata": {},
   "source": [
    "Using a ton of theory we cannot get into right now, one can show that the cross-validated error in using a $K$-bin histogram estimator is given by:\n",
    "\n",
    "$$\n",
    "E(K) = \\frac{2}{\\delta x (N_{D}-1)} - \\delta x\\frac{N_{D}+1}{N_{D}-1}\\sum_{l=1}^{K}\\left(\\frac{\\hat{p}_{l}}{\\delta x}\\right)^{2}\n",
    "$$\n",
    "\n",
    "Implement this and then explore how the cross-validated error changes as a function of bin count. Can you find an optimal bin count for the given data set?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7853309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(bincnt, samples):\n",
    "    nd = samples.size\n",
    "    dx = # add code here \n",
    "    hist, bins = np.histogram(samples, bins=bincnt, density=True)\n",
    "    error = # add code here \n",
    "    return bins, hist, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4229aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bincnts = np.arange(1, 100) # Explore bin counts \n",
    "errors = np.zeros(len(bincnts))\n",
    "for jj, bincnt in enumerate(bincnts):\n",
    "    bins, hist, error = cross_val(bincnt, vals2)\n",
    "    errors[jj] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bincnts, (errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vals2, bins=, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779d8a3-bdf0-48be-8bfe-0bf6cf77ac1f",
   "metadata": {},
   "source": [
    "**Problem 3**: Now lets build theory around our experimental results.  \n",
    "* Show that $\\int_{a}^{b}\\hat{p}_{H}(x)dx = 1$\n",
    "* Show that the probability of being in the interval $[a_{l},b_{l}]$ is given by $p_{l}=\\int_{a_{l}}^{b_{l}}p(x)dx$.\n",
    "* (Graduate) Show that the probability that there are $N_{l}$ particles in $[a_{l},b_{l}]$ is given by $\\tilde{p}_{l}(N_{l})$ where\n",
    "$$\n",
    "\\tilde{p}_{l}(N_{l}) = \\begin{pmatrix}N_{D}\\\\N_{l}\\end{pmatrix}p^{N_{l}}_{l}(1-p_{l})^{N_{D}-N_{l}}\n",
    "$$\n",
    "* Show, using the prior result, that \n",
    "$$\n",
    "\\sum_{N_{l}=0}^{N_{D}}\\tilde{p}_{l}(N_{l}) = 1.\n",
    "$$\n",
    "* (Graduate) Show that \n",
    "$$\n",
    "\\mathbb{E}[N_{l}] = \\sum_{N_{l}=0}^{N_{D}}N_{l}\\tilde{p}_{l}(N_{l}) = N_{D}p_{l}\n",
    "$$\n",
    "* * To do this, use the fact that \n",
    "$$\n",
    "N_{l}\\begin{pmatrix}N_{D}\\\\N_{l}\\end{pmatrix} = N_{D}\\begin{pmatrix}N_{D}-1\\\\N_{l}-1\\end{pmatrix}\n",
    "$$\n",
    "* Using all of the above results, show that for $x\\in[a_{l},b_{l}]$ that \n",
    "$$\n",
    "\\mathbb{E}[\\hat{p}_{H}(x)] = \\frac{p_{l}}{\\delta x}\n",
    "$$\n",
    "* What is the corresponding bias in your estimate?  \n",
    "* (Graduate) Now show that \n",
    "$$\n",
    "\\mathbb{E}[N_{l}^{2}] = N_{D}(N_{D}-1)p^{2}_{l} + N_{D}p_{l}\n",
    "$$\n",
    "\n",
    "* Using all of the above results, show that for $x\\in [a_{l},b_{l}]$ that $\\mathbb{V}[\\hat{p}_{H}(x)]$, where $\\mathbb{V}[\\hat{p}_{H}(x)] = \\mathbb{E}[(\\hat{p}_{H}(x) - \\mathbb{E}[\\hat{p}_{H}(x)])^{2}]$ is given by\n",
    "$$\n",
    "\\mathbb{V}[\\hat{p}_{H}(x)] = \\frac{1}{(\\delta x)^{2}N_{D}}p_{l}(1-p_{l})\n",
    "$$\n",
    "\n",
    "* In words, explain how the interval width $\\delta x$ controls the bias/variance tradeoff in our histogram estimator.  How does this better help you understand your empirical results from the prior problems?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498524b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
